# -*- coding: utf-8 -*-
"""segformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_t3KvF3qg4IJfEhTuftFI1GSlscapNgf
"""


from torch.utils.data import Dataset, DataLoader
from transformers import AdamW
import torch, math
from torch import nn
from sklearn.metrics import accuracy_score
from tqdm import tqdm
import os, sys
from PIL import Image
from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor, SegformerConfig, SegformerModel, PretrainedConfig
import pandas as pd
import cv2
import numpy as np
import albumentations as aug
import pdb
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from tools.metrics import ConfusionMatrix, IntersectionOverUnion, OverallAccuracy, F1Score, get_acc_v2
from tools.parse_config_yaml import parse_yaml_segFormer
import tools.transform as tr
import torchvision.transforms as standard_transforms
from scipy import ndimage as ndi
from skimage.color import label2rgb

# Regularization to avoid overfitting when training a learner with an iterative method
class EarlyStopper:
    def __init__(self, patience=1, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.min_validation_iou = -1000

    def early_stop(self, validation_iou):
        if validation_iou > self.min_validation_iou:
            self.min_validation_iou = validation_iou
            self.counter = 0
        elif validation_iou < (self.min_validation_iou + self.min_delta):
            self.counter += 1
            if self.counter >= self.patience:
                return True
        return False

class ImageSegmentationDataset(Dataset):
    """Read dataset."""

    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):
        """
        Args:
            root_dir (string): Root directory of the dataset containing the images + annotations.
            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.
            train (bool): Whether to load "training" or "validation" images + annotations.
        """
        self.root_dir = root_dir
        self.feature_extractor = feature_extractor
        self.train = train
        self.transforms = transforms

        sub_path = "train" if self.train else "val"
        self.img_dir = os.path.join(self.root_dir, sub_path, "images")
        self.ann_dir = os.path.join(self.root_dir, sub_path, "occ_masks")        
        
        # read images
        image_file_names = []
        for root, dirs, files in os.walk(self.img_dir):
            image_file_names.extend(files)
        self.images = sorted(image_file_names)
        
        # read annotations
        annotation_file_names = []
        for root, dirs, files in os.walk(self.ann_dir):
            annotation_file_names.extend(files)
        self.annotations = sorted(annotation_file_names)        

        assert len(self.images) == len(self.annotations) , "There must be as many images as there are segmentation maps"

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        
        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))
        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)
        segmentation_map = np.where(segmentation_map>0, 1, 0)
        

        sample = {'image': image, 'occ': segmentation_map}

        if self.transforms is not None:
            
            augmented = self.transforms(image=image, mask=segmentation_map) 
            encoded_inputs = self.feature_extractor(augmented['image'], augmented['mask'], return_tensors="pt")

            #augmented = self.transforms(sample)
            #encoded_inputs = self.feature_extractor(augmented['image'], augmented['occ'], return_tensors="pt")
                       
        else:
            encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors="pt")

        for k,v in encoded_inputs.items():
            encoded_inputs[k].squeeze_() # remove batch dimension        
        
        output = {'pixel_values': encoded_inputs['pixel_values'], 'labels_occ': encoded_inputs['labels'] }

        return output


class global_var():
    def __init__(self):
        self.var = []

    def set_var(self, var):
        self.var = var

    def get_var(self):
        return self.var

class InferenceImageSegmentationDataset(Dataset):
    """Read inference dataset."""

    def __init__(self, root_dir, feature_extractor, g_vars, transforms=None):
        """
        Args:
            root_dir (string): Root directory of the dataset containing the images + annotations.
            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.
            train (bool): Whether to load "training" or "validation" images + annotations.
        """
        self.root_dir = root_dir
        self.feature_extractor = feature_extractor
        self.transforms = transforms
        

        sub_path = "test" 
        self.img_dir = os.path.join(self.root_dir, sub_path, "images")
        self.ann_dir = os.path.join(self.root_dir, sub_path, "occ_masks")
        
        # read images
        image_file_names = []
        for root, dirs, files in os.walk(self.img_dir):
            image_file_names.extend(files)
        self.images = sorted(image_file_names)
        
        # read annotations
        annotation_file_names = []
        for root, dirs, files in os.walk(self.ann_dir):
            annotation_file_names.extend(files)
        self.annotations = sorted(annotation_file_names)

        
        g_vars.set_var(self.annotations)
        
        assert len(self.images) == len(self.annotations), "There must be as many images as there are segmentation maps"

    def __len__(self):
        return len(self.images)
    
    def get_annotations(self):
        return self.annotations

    def __getitem__(self, idx):
        
        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.resize(image, (512, 512), interpolation=cv2.INTER_NEAREST)
        
        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))
        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)
        segmentation_map = cv2.resize(segmentation_map, (512, 512), interpolation=cv2.INTER_NEAREST)
        segmentation_map = np.where(segmentation_map>0, 1, 0)

        sample = {'image': image, 'occ': segmentation_map}

        
        if self.transforms is not None:
            augmented = self.transforms(sample)
            encoded_inputs = self.feature_extractor(augmented['image'], augmented['occ'], return_tensors="pt")
        else:
            encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors="pt")

        #encoded_inputs.squeeze_() # remove batch dimension
        
        for k,v in encoded_inputs.items():
            encoded_inputs[k].squeeze_() # remove batch dimension
        
        output = {'pixel_values': encoded_inputs['pixel_values'], 'labels_occ': encoded_inputs['labels'], 'images': image}
       
        return output

def count_parameters(model):
            return sum(p.numel() for p in model.parameters() if p.requires_grad)


def train():

    start_epoch = 1

    root_dir = param_dict['root_dir']
    feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)

    train_dataset = ImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, transforms=transform)
    valid_dataset = ImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, transforms=transform, train=False)

    print("Number of training examples:", len(train_dataset))
    print("Number of validation examples:", len(valid_dataset))


    train_dataloader = DataLoader(train_dataset, batch_size=param_dict['batch_size'], shuffle=True)
    valid_dataloader = DataLoader(valid_dataset, batch_size=param_dict['batch_size'])
    
    # Weights for CE loss 
    #visible windows torch.tensor([0.60757092, 2.8240482])
    #occlusions [0.57487292, 3.83899089]
    criterion_occ  = nn.CrossEntropyLoss(weight=torch.tensor([0.57487292, 3.83899089])).cuda()
    criterion  = nn.CrossEntropyLoss(weight=torch.tensor([0.63147511, 2.40150057])).cuda()
    


    batch = next(iter(train_dataloader))

    for k,v in batch.items():
        print(k, v.shape)

    model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model_name_or_path = pre_model, ignore_mismatched_sizes=True, config = configuration_b2)

    if len(gpu_list) > 1:
        print('gpu>1')  
        model = torch.nn.DataParallel(model, device_ids=gpu_list)

    optimizer = AdamW(model.parameters(), lr=lr)    
    model.cuda()
    
    
    print("Model Initialized!")
    best_val_acc = 0.0
    print('Amount of Parameters: ', count_parameters(model))
    with open(os.path.join(param_dict['save_dir_model'], 'log.txt'), 'w') as ff:
        for epoch in range(start_epoch, param_dict['epoches']+1): 
            print("Epoch:", epoch)
            pbar = tqdm(train_dataloader)
            accuracies = []
            losses = []            
            val_accuracies = []
            val_losses = []
            model.train()
            for idx, batch in enumerate(pbar):
                
                # get the inputs
                pixel_values = batch["pixel_values"].cuda()
                labels_occ = batch["labels_occ"].cuda()

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward                
                outputs = model(pixel_values=pixel_values)
                            
                # Occlusion segmentation
                upsampled_logits_occ = nn.functional.interpolate(outputs.logits, size=labels_occ.shape[-2:], mode="bilinear", align_corners=False)
                loss_occ = criterion_occ(upsampled_logits_occ, labels_occ)
                predicted_occ = upsampled_logits_occ.argmax(dim=1)

                #mask = (labels != 255) # don't include the background class in the accuracy calculation
                pred_labels_occ = predicted_occ.detach().cpu().numpy()
                true_labels_occ = labels_occ.detach().cpu().numpy()
                                
                #accuracy = accuracy_score(pred_labels, true_labels)
                CM = ConfusionMatrix(2, pred_labels_occ, true_labels_occ) 
                IoU = IntersectionOverUnion(CM)
                OA = OverallAccuracy(CM)

                #---------------------------------------------------------------------------------                           

                loss = loss_occ.sum()         
                losses.append(loss)

                # backward + optimize
                loss.sum().backward()
                optimizer.step()
                
                pbar.set_postfix({'Batch': idx, 'Acc': OA, 'IoU':IoU, 'Total Loss': sum(losses)/len(losses)})                
            
            #lr_schedule.step()

            if epoch % param_dict['save_iter'] == 0:

                #### Validation ######
                model.eval()
                with torch.no_grad():
                    for idx, batch in enumerate(valid_dataloader):
                        pixel_values = batch["pixel_values"].cuda()                        
                        labels_occ = batch["labels_occ"].cuda()

                        outputs = model(pixel_values=pixel_values)
                        

                        # Occlusion segmentation
                        upsampled_logits_occ = nn.functional.interpolate(outputs.logits, size=labels_occ.shape[-2:], mode="bilinear", align_corners=False)
                        loss_occ_val = criterion_occ(upsampled_logits_occ, labels_occ)
                        predicted_occ = upsampled_logits_occ.argmax(dim=1)

                        #mask = (labels != 255) # don't include the background class in the accuracy calculation
                        pred_labels_occ = predicted_occ.detach().cpu().numpy()
                        true_labels_occ = labels_occ.detach().cpu().numpy()
                                        
                        #accuracy = accuracy_score(pred_labels, true_labels)
                        CM = ConfusionMatrix(2, pred_labels_occ, true_labels_occ) 
                        val_IoU = IntersectionOverUnion(CM)
                        OA = OverallAccuracy(CM)
                        F1 = F1Score(CM)

                        val_loss = loss_occ_val
                        val_losses.append(val_loss.sum())

                #### Validation ######
                   
                if epoch >= 20:

                    checkpoint = {
                                "net": model.state_dict(),
                                'optimizer': optimizer.state_dict(),
                                "epoch": epoch
                            }

                    if val_IoU[1] > best_val_acc:
                        name= str(epoch)+'valiou_best.pth'
                    else:
                        name = str(epoch)+'model.pth'
            
                    torch.save(checkpoint, os.path.join(model_dir, name))
                    best_val_acc = val_IoU[1]

                    if early_stopper.early_stop(best_val_acc):
                        print('Early stop break')             
                        break


                t_loss= sum(losses)/len(losses)
                v_loss = sum(val_losses)/len(val_losses)
                print(f"Train Loss: {t_loss}, Val Loss: {v_loss}, Val IoU: {val_IoU},")
                
                cur_log = 'epoch:{}, learning_rate:{}, train_loss:{}, val_loss:{}, val_f1:{}, val_acc:{}, val_miou:{}\n'.format(
                            str(epoch), str(lr), str(t_loss.item()), str(v_loss.item()), str(F1[1]),
                            str(OA),
                            str(val_IoU[1])
                        )
                
                ff.writelines(str(cur_log))                



# Inference

def inference():
    
    if os.path.exists(os.path.join(model_dir, 'test_occ')) is False:
            os.mkdir(os.path.join(model_dir, 'test_occ'))            
    
    palette = np.array([[  0,   0,   0], [255,255,255]])

    
    checkpoint_path = param_dict['checkpoint_inference']
    state_dict = torch.load(checkpoint_path)['net']
    model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model_name_or_path = pre_model, ignore_mismatched_sizes=True, config = configuration_b2)
    if len(gpu_list) > 1:
        print('gpu>1')  
        model = torch.nn.DataParallel(model, device_ids=gpu_list)

    #model = torch.nn.DataParallel(model, device_ids=[0])
    model.load_state_dict(state_dict)
    print('epoch: ', torch.load(checkpoint_path)['epoch'])
    

    model = model.cuda()
    root_dir = param_dict['root_dir']
    
    feature_extractor_inference = SegformerFeatureExtractor(do_random_crop=False, do_pad=False)
    g_vars = global_var()
    
    test_dataset = InferenceImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor_inference, g_vars= g_vars)
    annotations = g_vars.get_var()
    test_dataloader = DataLoader(test_dataset, batch_size=param_dict['batch_size'], shuffle=False) 
    
    test_num = len(annotations)
    label_all = np.zeros((test_num,) + (param_dict['img_size'], param_dict['img_size']), np.uint8)
    predict_all = np.zeros((test_num,) + (param_dict['img_size'], param_dict['img_size']), np.uint8)

    label_all_occ = np.zeros((test_num,) + (param_dict['img_size'], param_dict['img_size']), np.uint8)
    predict_all_occ = np.zeros((test_num,) + (param_dict['img_size'], param_dict['img_size']), np.uint8)

    
    batch = next(iter(test_dataloader))    

    model.eval()
    pbar = tqdm(test_dataloader)
    n = 0
    m = 0

    optimizer = AdamW(model.parameters(), lr=lr)

    optimizer.zero_grad()

    for idx, batch in enumerate(pbar):

        # get the inputs
    
        pixel_values = batch["pixel_values"].cuda() #images
        labels_occ = batch["labels_occ"].cuda() # occlusions
        
        outputs = model(pixel_values=pixel_values, output_attentions=True)        

        logits = outputs.logits.cpu()        
        print('logits.shape: ', logits.shape)        

        # Occlusions ------------------------------------------------
        upsampled_logits_occ = nn.functional.interpolate(logits,
                        size=labels_occ.shape[-2:], 
                        mode='bilinear',
                        align_corners=False)  
        _seg_occ = upsampled_logits_occ.argmax(dim=1)
        
        for im in range(batch["pixel_values"].shape[0]):
            seg = _seg_occ[im]
            label = labels_occ[im]

            color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) 
            label_all_occ[m] = seg            
            predict_all_occ[m] = label.cpu()
            
            for label, color in enumerate(palette):
                color_seg[seg == label, :] = color
            
            color_seg = color_seg[..., ::-1]
                                        
            ori_label = cv2.imread(os.path.join(root_dir, 'test', 'labels',annotations[m] ) ) 
            images = cv2.resize(np.asarray(batch["images"][im]).astype('uint8'), (ori_label.shape[1], ori_label.shape[0]), interpolation=cv2.INTER_NEAREST)

            labeled_windows, _ = ndi.label(seg)
            labeled_windows = cv2.resize(labeled_windows.astype('uint8'), (ori_label.shape[1], ori_label.shape[0]), interpolation=cv2.INTER_NEAREST)
            
                
            color_seg = label2rgb(labeled_windows, image=images)*255 
            cv2.imwrite(os.path.join(model_dir, 'test_occ', annotations[m]), (color_seg).astype(float))            
            m +=1
        

    get_acc_v2(
            label_all_occ, predict_all_occ,
            param_dict['num_class'] + 1 if param_dict['num_class'] == 1 else param_dict['num_class'],
            model_dir)



# Read config file

if len(sys.argv) == 1:
    yaml_file = 'config-SegFormer.yaml'
else:
    yaml_file = sys.argv[1]
param_dict = parse_yaml_segFormer(yaml_file)

for kv in param_dict.items():
    print(kv)

# Set params from config file
lr = param_dict['base_lr'] #0.00006  
model_dir = param_dict['save_dir_model']
pre_model = param_dict['pretrained_model']
gpu= param_dict['gpu_id'] 

# Set GPU device
os.environ["CUDA_VISIBLE_DEVICES"] = gpu
gpu_list = [i for i in range(len(gpu.split(',')))]
gx = torch.cuda.device_count()
print('useful gpu count is {}'.format(gx))

# Set early stop strategy
early_stopper = EarlyStopper(patience=param_dict['stop_pat'], min_delta=param_dict['stop_delta'])

# Set parameters for data trasnformations
transform = aug.Compose([
    aug.Flip(p=0.5),
    aug.HorizontalFlip(p=0.5),
    aug.geometric.rotate.Rotate (limit=[-15, 15])
    #aug.RandomCrop(width=128, height=128), does not work for this dataset and task    
    #aug.Normalize(mean=param_dict['mean'], std=param_dict['std']),    
])

# Set transformations for training dataset
transform_train = standard_transforms.Compose([ 
            tr.RandomHorizontalFlip(),
            tr.RandomVerticalFlip(),
            tr.ScaleNRotate(rots=(-15, 15), scales=(0.9, 1.1)),
            tr.FixedResize(param_dict['img_size'])
            #tr.Normalize(mean=param_dict['mean'], std=param_dict['std'])
            ]) 

# Set transformations for validation dataset            
transform_val = standard_transforms.Compose([
        tr.FixedResize(param_dict['img_size']),
        #tr.Normalize(mean=param_dict['mean'], std=param_dict['std'])
        ]) 

# Fixed configuration for SegFormer-B2
configuration_b2 = PretrainedConfig(            
            architectures = ["SegformerForSemanticSegmentation"],
            attention_probs_dropout_prob = 0.0,
            classifier_dropout_prob = 0.1,
            decoder_hidden_size = 768,  
            depths = [3, 4, 6, 3], 
            downsampling_rates = [1, 4, 8, 16],
            drop_path_rate = 0.1,
            hidden_act = 'gelu',
            hidden_dropout_prob = 0.0,
            hidden_sizes = [64, 128, 320, 512], 
            id2label = { "0": "background", "1": "occlusion"},
            image_size = 224, 
            initializer_range = 0.02,
            label2id = { "background": 0, "occlusion": 1 },
            layer_norm_eps = 1e-06,
            mlp_ratios = [4, 4, 4, 4],
            model_type = "segformer",
            num_attention_heads = [1, 2, 5, 8], 
            num_channels = 3, 
            num_encoder_blocks = 4, 
            patch_sizes = [7, 3, 3, 3], 
            reshape_last_stage = True,
            semantic_loss_ignore_index = 255,
            sr_ratios = [8, 4, 2, 1], 
            strides = [4, 2, 2, 2], 
            ignore_mismatched_sizes=True,
            torch_dtype = "float32",
            transformers_version = "4.18.0"
            ) 

# Run training or testing according to config file
if param_dict['mode'] == 'train':
    print('Training')
    train()
else:
    inference()






